---
layout: post
title: 【Paper】 SSD(SIngle Shot Multibox Detector)
description: >
     One Stage Detector의 시작이라고 할 수 있는 Yolo, SSD에 대해서 다시 공부하고 정리해본다.
---

SSD(SIngle Shot Multibox Detector)

## 1. Objecet Detection History 그리고 SSD

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91797915-ba9aa780-ec5e-11ea-9e25-9fbc3dc9e7e1.png' alt='drawing' width='600'/></p>

- Region Proposal + Detection 을 한방에 하는 One Stage detector
- Yolo가 처음 나와서 Faster RCNN보다 4배 속도는 빠르지만 정확도는 떨어졌다. 
- SSD는 Faster RCNN보다 mAP보다 좋고, Yolo보다 빠른 object detection 모델이다. 한때 SOTA
- 그 후 Yolo에서 SSD 기법을 차용하며 지금은 version 4 까지 나와서 속도와 정확도 모두를 가진 모델이 되었다. 

- SSD Network의 핵심 
    - BackBone을 통과시키고 점점 featrue map resolution을 낮춰가면서 그 모든 Feature map에 대해서 Detection + Box regressoin을 수행한다. 
    - 핵심은 2가지이다
        1. Multi Scale Feature Layer
        2. Default (Anchor) Box : 수행 성능과 속도의 2마리 토끼를 잡게 해준 포인트!


## 2. Multi Scale Feature Layer

- 과거 Multi Scale Detector = Image Pyramid를 이용하여 Sliding Windows 기법을 수행한다. 

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91798494-f97d2d00-ec5f-11ea-9140-10fa8f8129ce.png' alt='drawing' width='400'/></p>

- SSD는 이 Image Pyramid 기법을 차용해서 Object Detection을 수행한다. Feature Map을 convNet을 통과시켜 크기를 줄이고 줄여가며 각각 Detection을 수행한다.
- 아래의 이미지에서 32x32에서는 작은 object,  4x4에서는 큰 object를 Detect 할 수 있다. 

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91798697-70b2c100-ec60-11ea-85e0-a3c77453fddf.png' alt='drawing' width='400'/></p>


## 3. Default (Anchor) Box

- Faster RCNN의 RPN Network에 대해서 다시 생각해보자. Anchor 박스를 잘 활용해서 좋은 Detection을 수행할 수 있었다. 하지만 RPN에서도 그리고 Classification의 뒷단(Object Detection)에서도 Bounding Box Regression이 2번 수행되는 것은 약간 불 필요하지 않나? 라는 생각을 할 수 있다.

- Anchor Box 개념은 One stage Detector에서도 좋은 성능 향상을 가져와 주었다. 

- Anchor Box 개념을 RPN 뿐만 아니라 뒷단의 Object Detection에서도 수행하면 좋지 않을까? 라는 생각을 하게 되었다. 그래서 Yolo와 SSD에서 Default Anchor Box를 사용하게 되었다. 

- Anchor 박스를 통해서 학습해야 하는 것. 
    1. Anchor box와 겹치는 Feature 맵 영역의 obeject Class 분류 
    2. GT Box 위치를 예측할 수 있도록 수정된 좌표

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91800607-406d2180-ec64-11ea-91f4-7ce8d3579c79.png' alt='drawing' width='800'/></p>

- 위의 사진에서 1 ~ 6 번까지 흐름 순서대로 따라가며 공부하기  
- 확실히 이해가 안되면 [이번 Faster RCNN - Anchor box POST](https://junha1125.github.io/artificial-intelligence/2020-08-15-1FastRCNN/#1-2-rpn-%EC%83%81%EC%84%B8-%EC%84%A4%EB%AA%85%EA%B3%BC-loss-%ED%95%A8%EC%88%98) 확인해서 먼저 공부하고 다시 위의 사진 공부하기

## 4. SSD architecture 그리고 Default Box

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91809916-e5d5c480-ec67-11ea-8b61-5fa665243258.png' alt='drawing' width='600'/></p>

- SSD input은 512x512 or 300x300 이다. 
- Object Detion 단으로 넘어가는 Feature Map의 크기가 38x38 19x19 10x10 5x5 3x3 1x1 Feature map 결과들이다. 
- SSD 에서 논문에서는 **Dafault Box**라는 용어를 사용한다. **Anchor Box**와 똑같은 것이다. 

- 위 사진의 그림 중 윗 중간 부분에 Classifier : Conv, 3x3x(4x(classes+4)) 라고 적혀 있는 것을 확인할 수 있다. 이것에 대해서 자세하게 생각해보면 다음과 같다. 
    - **3x3**x(4x(classes+4)) : 3x3 필터를 사용한다. 위,위 사진을 확인하면 Locallization과 classification에서 3x3 필터를 사용하는 것처럼.
    - 3x3x(**4**x(classes+4)) : 4개의 Anchor 박스 
    - 3x3x(4x(**classes+4**)) : 각 class에 대한 Softmax 값(= confidence 값)(20+1(classes And background)), Bounding Box 좌표값.

    -  3x3x(**4x(classes+4)**) 이것은 Kernel의 갯수이자, Detection 결과 Output의 Depth 값이다. (ouput 결과의 width hight는 38x38 19x19 10x10 5x5 3x3 1x1 일 것이다.) 당연히 3x3 Kernel의 Depth는 각 Feature Map(38x38 19x19 10x10 5x5 3x3 1x1)의 Depth값을 따르게 된다. 

- 그렇게 해서 만들어지는 Box의 갯수는 다음과 같고 이것을 NMS 처리를 한다. 

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91812764-9f369900-ec6c-11ea-80ea-2c66282b098f.png' alt='drawing' width='500'/></p>

## 5. Matching 전략과 Loss 함수

- Matching 전략 : Ground True Bounding box와 겹치는 IOU가 0.5이상인 Anchor Box들을 찾는다. 이 Anchor Box를 Matching 된 Anchor Box라고 한다. 그런 Matching Anchor Box 만을 이용해서 Classification과 Bounding Box Regression에 대한 학습을 해나간다.

- Loss 함수 : 
    - Multi Tast Loss를 사용했다. Faster RCNN의 Loss와 동일하다.
    - N matching 된 Default Box의 갯수로써, 정규화에 사용한다. 

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91823970-fb052000-ec74-11ea-889d-cab81b43b900.png' alt='drawing' width='600'/></p>

## 6. Design Choice 별 Performance (아래 이미지 참조)
- 1:2 rate의 box를 사용했을 때 vs 1:3 rate의 box를 사용했을 때

- atrous : [dilated convolution](https://zzsza.github.io/data/2018/02/23/introduction-convolution/)

- Data Augmentation을 했을 때의 역할이 매우 컸다. 

- Yolo에서의 작은 Object Detection을 못하는 문제를 해결하기 위해 노력했다.   
    이 문제를 SSD에서는 Data Augmentation을 이용해서 해결하려고 엄청 노력을 했다. 

- 논문에서 계속 나오는 용어
    - SSD300 : 300x300 이미지 Input
    - SSD512 : 512x512 이미지 Input

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91824467-ccd41000-ec75-11ea-8360-d074c1c9de5c.png' alt='drawing' width='500'/></p>
    