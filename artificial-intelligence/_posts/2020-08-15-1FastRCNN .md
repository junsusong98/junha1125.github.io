---
layout: post
title: 【Paper】 Faster RCNN 개념 정리 + OpenCV DNN 모듈 기본
description: >
  Faster RCNN 개념을 다시 상기하고 정리해보고, 햇갈렸던 내용을 다시 공부해볼 예정이다.
---

Faster RCNN 개념, OpenCV Detection 구현

# 1. Faster RCNN
- Faster RCNN = RPN(Region Proposal Network) + Fast RCNN
- Region Proposal도 Network를 이용해서 찾는다.

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91658450-22d47680-eb03-11ea-9524-24311d6c43c4.png' alt='drawing' width='600'/></p>

- ROI Pooing은 동일하다. (Mask RCNN : Align ROI Pooling)
- RPN 에서도 Classification, Regression 모두가 들어가 있다. Classification는 객체의 유무만을 판단하고, Regression은 각 그리드 위치에 따른 객체가 있을 법한 위치를 뽑아준다. 이곳의 목표는 Object 유무와 위치를 **대강** 뽑아주는 것이다. 
- 위에서 대강 뽑은 Region Proposal에 대헤서 이후 끝단에 Classification, Regression을 해준다. 

## 1-1 RPN과 Anchor Box

RPN에서 주어진 입력 Pixel에 대해서 Selective Search 정도의 혹은 그 이상의 Region Proposal을 해주어야 한다. 다시 말해서 RPN 입장에서 우리가 가진 것은 데이터(Ground True Bounding Box)와 Backbone Network를 통과해서 나온 Feature Map 뿐이다. 이 2가지를 어떻게 이용해서 Region Proposal을 수행할 수 있을까?  

- Feature Map에 Anchor Box를 놓아두고 그곳에 Object가 있는지 없는지, Object의 위치는 좀 더 어디에 위치한지 알아본다. 

- Anchor Box는 9개로 구성되어 있다. (128x128 256x256 512x512) X (1:1 1:2 2:1)

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91658700-81025900-eb05-11ea-9f6c-3e4f99c4d988.png' alt='drawing' width='600'/></p>

- 위 이미지에 대해서 오른쪽 Feature Map의 하나의 1x1x512 pixel 값은 왼쪽 이미지의 16x16x3 pixel이 가지는 특징이 무엇인지에 대한 정보를 가지고 있다고 할 수 있다. (대충 그렇다는 것이다. 물론 Conv Layer를 통과하면서 Receptive Feild가 어땠는지는 다시 고려해봐야 할 것 이다.)

RPN의 구조
{:.lead}

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91674622-66b39400-eb74-11ea-8373-f0c12e4ff07c.png' alt='drawing' width='600'/></p>

- 참고로 RPN을 통과해서 나온는 Proposal Region x y w h 는 **대강** 추천해주는 Region Proposal 결과이다. 

 - RPN은 간단하다. Featrue Map에 (1 padding + 3x3 conv)  + (1x1 conv)를 하여 Classification, Box Regression 정보를 얻을 수 있다.

 - Softmax Classification에서 1 x 1 x 512 필터 하나가 하나의 엥커에 의해서 판단되는 결과(객체 유vs무)가 나와야 한다. 이 객체 유무의 값에서 '유'값(0~1값이) RPN에서의 Confidence값이라고 생각하면 된다. 

 - Bounding Box Regression에서 1 x 1 x 512 필터 하나가 하나의 엥커에 의해서 판단되는 결과(대강 물체 Region Proposal)가 나와야 한다. 

 - 참고 이미지 (알파프로젝트 하는 동안 Faster RCNN공부한 내용) 
  - m x n 에서 m x n 은 Feature Extractor를 통과후 나오는 Feature Map의 Resolution
  - 1 padding + 3x3 conv   ->   1x1 conv 




## 1-2 RPN 상세 설명과 Loss 함수

- Positive Anchor Box VS Negative Anchor Box
    - Ground True와 Bounding Box의 IOU값으로 분류 할 수 있다.
    - IOU 0.7 이상 : positive (Foreground를 학습)
    - IOU 0.3 이하 : Negative (Background를 학습)
    - IOU 0.3~0.7 사이 : 애매한 경우로 학습에 사용안함
    - Negative, Positive 만을 학습에 사용.

- 학습순서
    1. 실제 이미지에 Anchor 박스를 올려 놓는다!(위에서 2번쨰 사진처럼)
    2. Ground True랑 방금 올려놓은 Anchor 박스와의 Positive, Negative 정도를 판단한다. positive일 때만 아래의 Box regression학습을 한다. 
    3. Ground True랑 방금 올려놓은 Anchor 박스와의 Bounding Box Regression 정도를 파악하여 신경망을 학습시키고, RPN 신경망의 결과가 relative ofsset이 되도록 학습시킨다. (Anchor에 대해서, 어느정도로 이동하고 확장해야, Ground True Box로 근접하여 more positive가 될 수 있는지 신경망이 알도록 만든다. (Anchore이동으로 완벽히 Ground True가 될 필요는 없다. Anchor이동으로 Ground True에 대해서 more Positive가 되기만 하면 된다. ))
    4. 위의 2번과 3번에서 나온 결과값(A)이 RPN Layer를 통과하고 나온 결과(B), 즉 위에서 3번쨰 사진의 가장 오른쪽에 나오는 결과(B)가 되어야 한다. 
    5. 따라서 RPN의 Loss함수는 이렇다고 할 수 있다.   
        Loss Function = L1(A - B)

- 위에서 Loss 함수의 감을 잡았다면, 정확한 Loss 함수 수식을 살펴 보자. 
- pi,ti가 B이고 pi\*,ti\* 값이 A이다. pi는 Softmax값이고 pi\*는 1 or 0이다. 

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91675503-8f895880-eb77-11ea-90c5-6093cbc3d911.png' alt='drawing' width='500'/></p>

- 워낙 Negative Anchor가 많기 때문에 Positive Anchore를 항상 적절하게 섞어주어 학습을 진행해 주었다. 그냥 이미지 그대로 전체 학습을 시키면 Negative가 너무 많아서 학습이 잘 안된다고 한다. (Nagative Mining)

- 만약 모든 Positive Anchor가 Proposal된 Region이라고 하고, 이 모든 것들을 Head detector에 보내려면, 너무 많은 수의 Region을 보내야 한다. 따라서 다음과 같은 절차를 수행한다. 
    - Objectness Score를 계산 한다.  
      Objectness Score = pi값(Object일 확률 softmax값) * Ground True와 Bounding Box값의 IOU값
    - Objectness Score가 높은 순으로 오른차순 정렬을 한다. 
    - 오름차순된 Region Proposal에 대해서 몇개까지 만을 다음 Layer에 가져갈지 숫자인 N(다음 Detection Layer로 보낼 Region Proposal의 갯수)을 정한다. Ex 2000. 300. 50
    - 그것들만 2 stage인 classification, Box regression 단으로 가져가 학습에 사용한다. 

## 1-3 Alternation Training

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91681363-36c2bb80-eb89-11ea-98dd-f0fdf9c93a30.png' alt='drawing' width='600'/></p>

- 2개의 Network가 있다. 2개의 Network를 동시에 학습시키는 것이 쉽지 않다. 따라서 Training의 과정을 위의 사진과 같은 방법으로 해야한다. 여기서 1번과 2번은 Backbone과 같이 학습하고, 3번과 4번에서는 각각 1x1 conv, FC Layer만 좀만 더 Fine Turning 해준다. 

- 이런 방식으로 학습한 모델을 Inference 돌린 결과 실시간성 확보가 가능해지기 시작했다. 하지만 그래도 아직은 부족함이 존재했다. 
    - 1 stage detector 개발 필요
    - End to End 학습 모델 구축 필요

# 2. OpenCV를 이용한 Object Detection 개념
### 2-1 OpenCV DNN 장단점

- 장점
1. 딥러닝 개발 프레임 워크 없이 쉽게 Inferecne 구현 가능  
2. OpenCV에서 지원하는 다양한 Computer vision 처리 API와 Deep Learning을 쉽게 결합  
3. 인텔은 모바일 분야에 집중하여 CPU기반 OpenCV
4. NVIDIA - GPU 학습 혹은 추론 각각에서 좋은 성능을 가진 다양한 GPU개발되고 있음
5. TPU - 구글에서 개발. 

- 단점
1. GPU 지원이 약함  
2. Inference만 가능
3. CPU 기반 Inference 속도가 개선되었으나, GPU를 사용하는 것 대비 아직도 속도가 느림.  


### 2-2 Deep Learning Frame 사용방법    

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91684385-4db9db80-eb92-11ea-9934-ccb90bc3d1e8.png' alt='drawing' width='600'/></p>

- Yolo를 만든것이 DarkNet그룹이므로, OpenCV에서 Yolo를 사용하려면 DarkNet 회사에서 재공하는 가중치 모델 파일, 환경 파일을 넣어야 한다. 
- cvNet으로 Neural Network가 반환이 되어 그 변수를 사용할 수 있다. 

### 2-3 참고 Document 사이트    

  - [https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV](https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV)    
  - [https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API](https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API)  

<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91684734-41824e00-eb93-11ea-9d5d-a2af60ff1a58.png' alt='drawing' width='600'/></p>


### 2-4 OpenCV DNN을 이용하기 위한 Inference 수행 절차    

- 우리가 앞으로 코드에서 사용할 내용의 전체적인 큰 그림이다. 계속 사용할 것이므로, 잘 알아주자. 
<p align="center"><img src='https://user-images.githubusercontent.com/46951365/91684896-bb1a3c00-eb93-11ea-9290-8ddae3e677e6.png' alt='drawing' width='600'/></p>  

1. 가중치와 환경설정파일의 형식(.pb, .pbtxt)임에 주의하자.  
    1-1 그리고 이미지를 가져온다. 이미지를 가져오는 동안 bolbFromImage()라는 함수를 사용할 수 있다.  
    bolbFromImage() 의 역할 : Network에 들어갈 이미지로 사이즈 조정, 이미지 값 스케일링, BGR을 RGB로 변경. 이미지 Crop할 수 있는, 모든 옵션 제공  

    ```python
    img_bgr = cv2.imread('img path')
    cvNet.setInput(cv2.dnn.blobFromImage(img_bgr, size = (300,300), SwapRB = True, crop = False)
    # swapRB : BGR을 RGB로 변경
    # size는 내가 사용하는 CvNet에 따라 다른다. 내가 사용하고 싶은 네트워크가 무슨 사이즈의 Input을 받는지는 알아두어야 한다. 
    ```

    1-2 Vidoe Stream Capture 
      - OpenCV의 VideoCapture() API를 사용하면 Video Stream을 Frame별로 Capture한 Image에 대해서 Object Detection을 쉽게 수행할 수 있다.
      - [참고 이전 게시물](https://junha1125.github.io/artificial-intelligence/2020-08-12-OpenCV/)
      - 모든 곳에서 아주 유용하게 활용되고 있으니 잘 알아두자. 

2. 이미지를 cvNet에 넣는다. 

3. forward를 통해서 Detect된 output을 변수에 저장한다. 

4. Detection된 결과에 for를 통해서 하나하나의 Object를 확인해본다. 

### 2-5 코드 활용  

다음 Blog Post 참조







