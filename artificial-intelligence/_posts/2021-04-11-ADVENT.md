---
layout: post
title: 【DA】ADVENT-Entropy Minimization for DA
---

- **논문** : [ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/1811.12833)
- **분류** : Domain Adaptation
- **읽는 배경** : Domain Adaptation 기본, Adversarial code의 좋은 예시
- **느낀점** : 
- **참고 사이트** : [Github page](https://github.com/valeoai/ADVENT),
- **목차**
  1. 



---

---

# ADVENT

# 1. Conclusion, Abstract, Instruction

- Task: unsupervised domain adaptation for semantic segmentation
- Motivation: Real world와 비교해보면, Data distribution(데이터 분포, 이미지의 통계적 성질)의 많은 차이가 존재한다. 
- **Simple observation**
  - (1) Source이미지에 대해서, over-confident, low-entropy predictions
  - (2) Target이미지에 대해서,  under-confident, high-entropy predictions
  - 과거에는 Pseudo label을 사용해서 Self-Training을 하는 방법, Feature output에 대해서 Adversarial Learning을 적용하는 방법을 사용했지만, 우리는 Target도 high-confident를 예측하게 만듬으로써, domain gap을 줄이는 방법을 고안했다.
- **핵심 모듈** 
  - (1) an entropy loss: target 이미지가 high-confident를 예측하도록 강제한다. low-confident에 대해서 Loss를 준다.
  - (2) entropy-based adversarial training approach : AdaptSeg가 Output을 이용했다면, 여기서는 Output에 대해서 entropy를 계산한 map을 이용해서 Adversarial learning을 적용한다.
  - 추가적인 성능향상을 위해서, 아래의 기법을 적용했다.
    - (i) training on specific entropy ranges(Entropy_x_target ∈ [0, 1] (H×W) 
    - (ii) incorporating class-ratio priors. (그저 자주 나오는 class가 1 confident가 나와 버리는 문제를 막는다) 



---

# 3. Approaches

- ㅇ



---

# 4. Experiments

- ㅇ



---

# 5. Results

- ㅇ



---

---



