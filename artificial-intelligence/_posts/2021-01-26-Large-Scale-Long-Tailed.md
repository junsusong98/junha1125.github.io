---
layout: post
title: ã€ClassBlanceã€‘Large-Scale Long-Tailed Recognition in an Open World = OLTR w/ advice
---

- **ë…¼ë¬¸** : [Large-Scale Long-Tailed Recognition in an Open World - y2019-c103](https://arxiv.org/pdf/1904.05160.pdf)
- **ë¶„ë¥˜** : Unsupervised Domain Adaptation
- **ì €ì** : Ziwei Liu1,2âˆ— Zhongqi Miao2âˆ— Xiaohang Zhan1
- **ì½ëŠ” ë°°ê²½** : (citation step1) Open Componunt Domain Adaptationì—ì„œ Memory ê°œë…ì´ ì´í•´ê°€ ì•ˆë˜ì„œ ì½ëŠ” ë…¼ë¬¸. 
- **ì½ìœ¼ë©´ì„œ ìƒê°í•  í¬ì¸íŠ¸** : ë…¼ë¬¸ì´ ì–´ë–¤ íë¦„ìœ¼ë¡œ ì“°ì—¬ì¡ŒëŠ”ì§€ íŒŒì•…í•˜ì. ë‚´ê°€ ë‚˜ì¤‘ì— ì“¸ ìˆ˜ ìˆë„ë¡.
- **[ë™ì˜ìƒ ìë£Œ](https://www.youtube.com/watch?v=A45wrs1g8VA)** 
  - <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126160627433.png?raw=tru" alt="image-20210126160627433" style="zoom:80%;" />
- ì§ˆë¬¸
  - centroidsë©”ëª¨ë¦¬ Mì„ ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¤ëŠ”ì§€ëŠ” ì•„ì§ ëª¨ë¥´ê² ë‹¤. ì´ê²ƒì€ ì½”ë“œë¥¼ í†µí•´ì„œ ê³µë¶€í•˜ë©´ ë ë“¯.   
    [OLTR/models/MetaEmbeddingClassifier.py](OLTR/models/MetaEmbeddingClassifier.py ) íŒŒì¼ì— forwardì˜ inputìœ¼ë¡œ centroidsê°€ ë“¤ì–´ê°€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 
- ì„ ë°°ë‹˜ ì¡°ì–¸ 
  - ì™¸êµ­ì—ì„œ ì‚¬ëŠ”ê²Œ ê·¸ë¦¬ ì¢‹ì€ê²Œ ì•„ë‹ˆë‹¤. ìš°ë¦¬ë‚˜ë¼ ë¼ëŠ” ê²ƒì´ ì–¼ë§ˆë‚˜ í° **ì¶•ë³µ**ì¸ì§€ ëª¨ë¥¸ë‹¤. ê¸°íšŒê°€ ìˆë‹¤ë©´ ë‚˜ê°€ì„œ ì¼í•˜ê³  ë‚˜ì¤‘ì— ë‹¤ì‹œ ëŒì•„ì˜¤ë©´ ëœëŠ”ê±°ë‹¤. ìš°ë¦¬ë‚˜ë¼ì— ëŒ€í•´ì„œ ê°ì‚¬í•¨ì„ ê°€ì§€ê³  ì‚´ì•„ê°€ë„ë¡ í•´ì•¼ê² ë‹¤. 
  - íŠ¹íˆë‚˜ ì™¸êµ­ì—ì„œ ì˜¤ë˜ ì‚´ê³  ì˜¤ì…¨ê¸° ë•Œë¬¸ì—, ì € ì§„ì‹¬ìœ¼ë¡œ í•´ì£¼ì‹œëŠ” ì¡°ì–¸ì´ì—ˆë‹¤. ê·¸ëƒ¥ í° í™˜ìƒì„ ê°€ì§€ê³  ê±°ê¸°ì„œ ì‚´ê³  ì‹¶ë‹¤ ë¼ëŠ” ìƒê°ì„ í•´ë´¤ì í™˜ìƒì€ ê¹¨ì§€ê¸° ë§ˆë ¨ì´ê³ , ìš°ë¦¬ë‚˜ë¼ ì•ˆì—ì„œ, ìš°ë¦¬ë‚˜ë¼ê°€ ì£¼ëŠ” í¸ì•ˆí•¨ê³¼ í¬ê·¼í•¨ì— ê°ì‚¬í•˜ë©° ì‚´ì•„ê°€ëŠ”ê²Œ ì–¼ë§ˆë‚˜ í° ì¶•ë³µì¸ì§€ ì•Œë©´ ì¢‹ê² ë‹¤**. ëª¨ë¹„ìŠ¤ì™€ ë‹¤ë¥¸ ì™¸êµ­ ê¸°ì—…ê³¼ì˜ ë¹„êµë¥¼ ìƒê°í•˜ë©° ê°ì‚¬í•  ì¤„ ëª°ëë‹¤. í•˜ì§€ë§Œ ê°ì‚¬í•˜ì. ëª¨ë¹„ìŠ¤ì— ê°€ì„œë„ ì •ë§ ì—´ì‹¬íˆ ìµœì„ ì„ ë‹¤í•´, ìµœê³ ê°€ ë˜ê¸° ìœ„í•´ ê³µë¶€í•˜ì. ê·¸ë ‡ê²Œ í•´ì•¼ ì •ë§ ë„¤ì´ë²„ë“  í´ë¡œë²„ë“  ê°ˆ ìˆ˜ ìˆë‹¤. ê·¸ê²Œ ë‚´ê°€ ì •ë§ ê°€ì•¼í•  ê¸¸ì´ë‹¤. ê·¸ëŸ¬ë‹¤ê°€ ê¸°íšŒê°€ ë˜ì–´ ì™¸êµ­ ê¸°ì—…ì´ ë‚˜ë¥¼ ë¶€ë¥¸ë‹¤ë©´, ë‹¤ë…€ì˜¤ë©´ ëœë‹¤. ê·¸ë¦¬ê³  ë˜ í•œêµ­ì´ ê·¸ë¦¬ì›Œì§€ë©´ ë‹¤ì‹œ ëŒì•„ì˜¤ë©´ ë˜ëŠ”ê±°ë‹¤. ë‚˜ì˜ ë¯¸ë˜ë¥¼ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ì‹  ì„ ë°°ë‹˜ê»˜ ê°ì‚¬í•©ë‹ˆë‹¤.** 
  - í•™êµì—ì„œ ë„ˆë¬´ ë§ì€ ê²ƒì„ ë°°ìš°ë ¤ê³  í•˜ì§€ ë§ˆë¼. ìˆ˜ì—…ì€ ê·¸ëƒ¥ ì‰¬ìš´ê²Œ ì§±. í•˜ê³  ì‹¶ì€ ì—°êµ¬í•˜ê³  í•˜ê³  ì‹¶ì€ ê³µë¶€í•˜ëŠ”ê²Œ ìµœê³ ë‹¤. ê·¸ë¦¬ê³  ë™ê¸°ì™€ ì¹œêµ¬ì™€ ê°™ì´ ìˆ˜ì—… ë“£ëŠ” ê²ƒì„ ë” ì¶”ì²œí•œë‹¤.



# **ëŠë‚€ì **  

1. Instructionì´ ê°œê°™ì€ ë…¼ë¬¸ì€..
   - abstract ë¹ ë¥´ê²Œ ì½ê³ , Introduction ëŒ€ì¶© ì½ì–´ ë„˜ê²¨ì•¼ ê² ë‹¤. ë­”ì†Œë¦¬í•˜ëŠ”ì§€ ë„ì €íˆ!!!!!! ëª¨ë¥´ê² ë‹¤. 
   - ì§€ë‚´ë“¤ì´ í•œ ê³¼ì •ë“¤ì„ ìš”ì•½ì„ í•´ë†¨ëŠ”ë°.. ë‚˜ëŠ” ì •í™•íˆ ì•Œì§€ë„ ëª»í•˜ëŠ”ë° ìš”ì•½ë³¸ì„ ì½ìœ¼ë ¤ë‹ˆê¹Œ ë” ëª¨ë¥´ê² ë‹¤.
   - ë”°ë¼ì„œ ê·¸ëƒ¥ abstractì½ê³  introduction ëŒ€ì¶© ëª¨ë¥´ëŠ”ê±° ê± ë„˜ì–´ê°€ì„œ ì½ê³ . 
   - relative workì˜ ìƒˆë¡œìš´ ê°œë…ë§Œ ë¹ ë¥´ê²Œ í›‘ê³ , ë°”ë¡œ Ours Modelì— ëŒ€í•œ ë‚´ìš©ë“¤ì„ ë¨¼ì € ê¹Šê²Œ ì½ì. ê·¸ë¦¼ê³¼ í•¨ê»˜ ì´í•´í•˜ë ¤ê³  ë…¸ë ¥í•˜ë©´ì„œ. 
   - ê·¸ë¦¬ê³ ! Introduceì„ ë‹¤ì‹œ ì°¾ì•„ê°€(ğŸ‘‹) ì½ìœ¼ë©°, ë‚´ê°€ ê³µë¶€í–ˆë˜ ë‚´ìš©ë“¤ì˜ ìš”ì•½ë³¸ì„ ì½ì 
2. ì•„ë¬´ë¦¬ Abstract, Instruction, Relative workë¥¼ ì½ì–´ë„, ì´í•´ê°€ ë˜ëŠ” ì–‘ë„ ì •ë§ ì¡°ê¸ˆì´ê³  ë¨¸ë¦¬ì— ë‚¨ëŠ” ì–‘ë„ ì–¼ë§ˆ ë˜ì§€ ì•ŠëŠ”ë‹¤. ì§€ê¸ˆë„ ìœ„ 2ê°œì—ì„œ í•µì‹¬ì´ ë­ì˜€ëƒê³  ë¬¼ìœ¼ë©´, ëŒ€ë‹µ ëª»í•˜ê² ë‹¤. 
   - í˜„ì¬ì˜ ë¨¸ì‹ ëŸ¬ë‹ ë…¼ë¬¸ë“¤ì´ ë‹¤ ê·¸ëŸ°ê²ƒ ê°™ë‹¤. ê·¸ëƒ¥ ëŒ€ì¶© ì‹ ê²½ë§ì— ë•Œë ¤ ë„£ìœ¼ë‹ˆê¹Œ ì˜ëœë‹¤. 
   - í•˜ì§€ë§Œ ê·¸ ì´ìœ ëŠ” ì§ê´€ì ì¼ ë¿ì´ë‹¤. ë”°ë¼ì„œ ëŒ€ì¶© ì´ë ‡ë‹¤ì €ë ‡ë‹¤ ì‚ê¹Œë»”ì©í•œ ë§ë§Œ ì—„ì²­ ë„£ì–´ë‘”ë‹¤. ì´ëŸ¬ë‹ˆ ì´í•´ê°€ ì•ˆë˜ëŠ”ê²Œ ë„ˆë¬´ë‚˜ ë‹¹ì—°í•˜ë‹¤. 
   - ì´ëŸ° ì ì„ ê³ ë ¤í•´ì„œ, ì¢Œì ˆí•˜ì§€ ì•Šê³  ë…¼ë¬¸ì„ ì½ëŠ” ê²ƒë„ ë§¤ìš° ì¤‘ìš”í•œ ê²ƒ ê°™ë‹¤. (ğŸ‘‹)ì—¬ê¸° ì•„ì§ ì•ˆì½ì—ˆë‹¤ê³ ??? ê±±ì •í•˜ì§€ ë§ˆë¼. í•µì‹¬ Model ì„¤ëª… ë¶€ë¶„ ì½ê³  ì˜¤ë©´ ë” ì´í•´ ì˜ë˜ê³  ë¨¸ë¦¬ì— ë‚¨ëŠ”ê²Œ ë§ì„ê±°ë‹¤. í™”ì´íŒ….
3. **í™•ì‹¤íˆ Modelì— ë” ì§‘ì¤‘í•˜ë‹ˆê¹Œ, í›¨ì”¬ ì¢‹ì€ ê²ƒ ê°™ë‹¤. ì½”ë“œê¹Œì§€ í™•ì¸í•´ì„œ ê³µë¶€í•˜ë©´ ê¸ˆìƒì²¨í™”ì´ë‹¤. ì´ê±° ì´ì „ì— ì½ì€ ë…¼ë¬¸ë“¤ì€, ê·¸ ë…¼ë¬¸ë§Œì˜ ë°©ë²•ë¡ ì—ëŠ” ì§‘ì¤‘í•˜ì§€ ëª»í–ˆëŠ”ë°, ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ ê¼­! ë‹¤ì‹œ ì½ì–´ì„œ ì—¬ê¸° ì²˜ëŸ¼ ìì„¸íˆ ì •ë¦¬í•´ ë‘ì–´ì•¼ ê² ë‹¤.**
4. ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì€ ì´ë¯¸ íŒŒì•…í–ˆë‹¤. ğŸ‘‹ ì½ê¸° ì‹«ë‹¤. ì•ˆ ì½ê³  ì •ë¦¬ ì•ˆí–ˆìœ¼ë‹ˆ, ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ ì°¸ê³ í•˜ì. ëª…ì‹¬í•˜ì. ì½ì„ ë…¼ë¬¸ì€ ë§ë‹¤. ëª¨ë“  ë…¼ë¬¸ì„ ë‹¤ ì •í™•í•˜ê²Œ ì½ëŠ”ê²Œ ì¤‘ìš”í•œê²Œ ì•„ë‹ˆë‹¤.



# 0. Abstract

- the present & challenges
  1. Real world data often have a long-tailed and open-ended distribution. ì¦‰ ì•„ë˜ì˜ ê·¸ë˜í”„ì˜ xì¶•ì€ classì¢…ë¥˜ë¥¼ ë°ì´í„°ê°€ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•œ ê²ƒì´ê³ , yì¶•ì€ í•´ë‹¹ í´ë˜ìŠ¤ë¥¼ ê°€ì§€ëŠ” ë°ì´í„° ìˆ˜. ë¼ê³  í•  ìˆ˜ ìˆë‹¤. Open ClassëŠ” ìš°ë¦¬ê°€ êµ³ì´ Annotate í•˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ì´ë‹¤.
  2. <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126154310688.png?raw=tru" alt="image-20210126154310688" style="zoom: 80%;" />
- Ours - ì•„ë˜ ë‚´ìš© ìš”ì•½



# 1. Introduction

-  the past	
   -   ëŒ€ë¶€ë¶„ì˜ ê¸°ë²•ë“¤ì€ Head Class ì¸ì‹ì— ì§‘ì¤‘í•˜ì§€ë§Œ,   
      ìµœê·¼ Tail class ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ ì¡´ì¬í•œë‹¤. (=Few-shot Learning)for the small data of tail classes [52, 18]
   
- Ours
   -  ìš°ë¦¬ì˜ ê³¼ì œ(with one integrated algorithm)  
      1) imbalanced classification    
      2) few-shot learning  
      3) open-set recognition  
      ì¦‰. tail recognition robustness and open-set sensitivity:
      
   - Open Long-Tailed Recognition(OLTR) ì´ í•´ê²°í•´ì•¼í•˜ëŠ” ë¬¸ì œ  
     1) how to **share visual knowledge(=concept)** between **head and tail** classes (For **robustness**)  
     2) how to **reduce confusion** between **tail and open** classes (For **sensitivity**)     

   - í•´ê²°ë°©ë²• 

     - <u>2ë²ˆì§¸ í˜ì´ì§€ We develop an OLTR ë¬¸ë‹¨ë¶€í„° ë„ˆë¬´ ì´í•´ê°€ ì–´ë µë‹¤. ë”°ë¼ì„œ ì¼ë‹¨ íŒ¨ìŠ¤í•˜ê³  ë‹¤ì‹œ ì˜¤ì. ìš”ì•½ë³¸ì´ë‹ˆ, êµ¬ì²´ì ìœ¼ë¡œ ê³µë¶€í•˜ê³  ë‚˜ë©´ ì´í•´í•˜ê¸° ì‰¬ìš¸ ê±°ë‹¤. </u> ğŸ‘‹

     - ```sh
       # ì´ì „ì— ì •ë¦¬í•´ ë†“ì€ ìë£Œ. ë‚˜ì¤‘ì— ì™€ì„œ ë‹¤ì‹œ ì°¸ì¡°.
       1 mapping(=dynamic meta-embedding) an image to a feature space  = **a direct feature** computed(embeded) from the input image from the training data   
       2) visual concepts(=visual memory feature) ì´ ì„œë¡œì„œë¡œ ì—°ê´€ëœë‹¤.(associate with) = A visual memory holds **discriminative centroids**     
       3) A summary of **memory activations** from the direct feature   
       ê·¸ë¦¬ê³  combine into a meta-embedding that is enriched particularly for the tail class.
       ```
     
   -  .

   

   # 2. Related Works

   ![image-20210126181200150](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126181200150.png?raw=tru)

- **Imbalanced Classification**
  
  - Classical methods -  under-sampling head classes, over-sampling tail classes, and data instance re-weighting.
  -  Some recent methods - metric learning, hard negative mining, and meta learning.
  - Ours
    - combines the strengths of both metric learning[24, 37] and meta learning[17, 59]
    - Our dynamic meta-embedding~~~ ğŸ‘‹
  
- **Few-Shot Learning**
  
  -  ì´ˆê¸°ì˜ ë°©ë²•ë“¤ they often suffer a moderate performance drop for head classes.
  - ìƒˆë¡œìš´ ì‹œë„ : The few-shot learning without forgetting,  incremental few-shot learning.
  - í•˜ì§€ë§Œ : ìœ„ì˜ ëª¨ë“  ë°©ë²•ë“¤ì€, the training set are balanced.
  - In comparison, ours~~~ ğŸ‘‹
  
- **Open-Set(new data set) Recognition**
  
  -  OpenMax [3] : calibrating the output logits
  - OLTR approach incorporates~~ ğŸ‘‹



# 3. Our OLTR Model

![image-20210126184016363](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126184016363.png?raw=tru)

- ìš°ë¦¬ ëª¨ë¸ì˜ í•µì‹¬ì€ Modulated Attention ê·¸ë¦¬ê³  Dynamic meta-embedding ì´ë‹¤. 
  - dynamic **Embedding**  : visual concepts(**transfers knowledge**) between **head and tail** 
  - modulated **Attention** : **discriminates**(êµ¬ë¶„í•œë‹¤) between **head and tail**
  - **reachability** : **separates** between **tail and open**
- Our OLTR Model
  - We propose to **map(mappingí•˜ê¸°)** an image to a **feature space** /such that **visual concepts** can easily relate to each other /based on a **learned metric** /that respects the closed-world classification /while acknowledging the novelty of the open world.



## 3-1. Dynamic Meta-Embedding

- combines **a direct image feature** and **an associated memory feature** (with the feature norm indicating the **familiarity** to known classes)

  - CNN feature ì¶”ì¶œê¸° ê°€ì¥ ë§ˆì§€ë§‰ ë’· ë‹¨ì´ **V_direct**(linear vector=**direct feature**)ì´ë‹¤. (classificationì„ í•˜ê¸° ì§ì „)
  - tail classes(dataì–‘ì´ ë³„ë¡œ ì—†ëŠ” classì˜ ë°ì´í„°)ì—ëŠ” ì‚¬ì‹¤ V_directì´ ì¶©ë¶„í•œ featureë“¤ì´ ì¶”ì¶œë˜ì–´ ë‚˜ì˜¤ê¸° ì–´ë µë‹¤. ê·¸ë˜ì„œ tail dataì™€ ê°™ì€ ê²½ìš°, V_memory(**memory feature**) ì™€ ìœµí•©ë˜ì–´ enrich(ì¢€ë” sementicí•œ ì •ë³´ë¡œ ë§Œë“¤ê¸°) ëœë‹¤. ì´ V_memoryì—ëŠ” **visual concepts from training classes**ë¼ëŠ”ê²Œ ë“¤ì–´ê°€ ìˆë‹¤. 

- Learning Visual Memory (**M**)

  - [23] ì´ ë…¼ë¬¸ì˜ class **structure analysis** and adopt **discriminative centroids**  ë‚´ìš©ì„ ë”°ëë‹¤.
  - ![image-20210126205303346](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126205303346.png?raw=tru) ì—¬ê¸°ì„œ KëŠ” classì˜ ê°¯ìˆ˜ì´ë‹¤. 
  - Mì€ V_directì— ì˜í•´ì„œ í•™ìŠµì´ ëœë‹¤. centroids ì •ë³´ê°€ ê³„ì†ì ìœ¼ë¡œ Updateëœë‹¤ê³  í•œë‹¤. ì—¬ê¸°ì„œ centroidsì •ë³´ëŠ” ì•„ë˜ì˜ ì‚¼ê°í˜• ìœ„ì¹˜ì´ë‹¤. ì•„ë˜ì˜ ì‘ì€ ë™ê·¸ë¼ë¯¸ê°€ V_direct ì •ë³´ì´ê³ , ê·¸ê²ƒì˜ ì¤‘ì‹¬ì´ centroidsê°€ ëœë‹¤. 
  - ì´ centroidsëŠ” inter-classì— ëŒ€í•´ì„œ ê±°ë¦¬ê°€ ê°€ì¥ ê°€ê¹ê²Œ, intra-classì— ëŒ€í•´ì„œëŠ” ê±°ë¦¬ê°€ ìµœëŒ€í•œ ë©€ê²Œ ì„¤ì •í•œë‹¤.
  - <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126205524272.png?raw=tru" alt="image-20210126205524272" style="zoom:67%;" />
  - centroidsë¥¼ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë©´ ë˜ëŠ”ì§€ëŠ” ì½”**ë“œë¥¼ ì¢€ë§Œ ë” ë””ì ¸ë³´ë©´ ë‚˜ì˜¬ ë“¯**í•˜ë‹¤. ì•„ë˜ pythonì½”ë“œì˜ centroidsê°€ í•µì‹¬ì´ë‹¤. **centroids**ëŠ” modelì˜ forward ë§¤ê°œë³€ìˆ˜ë¡œ ë“¤ì–´ì˜¨ë‹¤. 

- Memory Feature (**V_memory**)

  - O : V_directì™€ ië²ˆì§¸ í´ë˜ìŠ¤ê°„ì˜ ìƒê´€ê³„ìˆ˜(coefficients hallucinated(ìƒê´€ê´€ê³„ë¼ê³  í™˜ê°ì´ ëŠê»´ì§€ëŠ” ë‹¨ìˆœí•œ Fully Conected Layer....))ë¥¼ ì˜ë¯¸í•œë‹¤. 

  - V_memoryëŠ” ì•„ë˜ì˜ ì½”ë“œì²˜ëŸ¼ Mê³¼ Oë¥¼ torch.matmulí•´ì„œ ë§Œë“¤ì–´ ë‚¸ë‹¤. <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126205902226.png?raw=tru" alt="image-20210126205902226" style="zoom: 67%;" />

  - [Github Link](https://github.com/zhmiao/OpenLongTailRecognition-OLTR/blob/master/models/MetaEmbeddingClassifier.py)

    - ```python
      # set up visual memory . [M ë§Œë“¤ê¸°]
      x_expand = x.clone().unsqueeze(1).expand(-1, self.num_classes, -1)
      centroids_expand = centroids.clone().unsqueeze(0).expand(batch_size, -1, -1)
      keys_memory = centroids.clone()
      
      # computing reachability
      dist_cur = torch.norm(x_expand - centroids_expand, 2, 2)
      values_nn, labels_nn = torch.sort(dist_cur, 1)
      scale = 10.0
      reachability = (scale / values_nn[:, 0]).unsqueeze(1).expand(-1, feat_size)
      
      # computing memory feature by querying and associating visual memory
      # self.fc_hallucinator = nn.Linear(feat_dim, num_classes)
      values_memory = self.fc_hallucinator(x.clone())
      values_memory = values_memory.softmax(dim=1)
      memory_feature = torch.matmul(values_memory, keys_memory)
      ```

- V_meta 

  - <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126212730673.png?raw=tru" alt="image-20210126212730673" style="zoom:80%;" />
  - V_metaì´ ì •ë³´ê°€ ë§ˆì§€ë§‰ classifierì— ë“¤ì–´ê°€ê²Œ ëœë‹¤.
  - <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126214602677.png?raw=tru" alt="image-20210126214602677" style="zoom:80%;" />
  - ì™¼ìª½ ì´ë¯¸ì§€ ì²˜ëŸ¼, ê·¸ëƒ¥ V_directë¥¼ ì‚¬ìš©í•˜ë©´, inter-classê°„ì˜ ê±°ë¦¬ê°€ ë©€ë¦¬ ë–¨ì–´ì§€ëŠ” ê²½ìš°ë„ ìƒê¸´ë‹¤.
  - ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì€, V_metaë¥¼ í™•ì¸í•œ ê²ƒì¸ë°, inter-classê°„ì˜ ê±°ë¦¬ê°€ ë” ê°€ê¹Œì›Œì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- Reachability (*Î³*)

  - closed-world classì— open-world classë¥¼ ì ìš©í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤.
  - ê³µì‹ì€ ì´ì™€ ê°™ê³ , <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126213933294.png?raw=tru" alt="image-20210126213933294" style="zoom: 67%;" />
  - ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”, class ì¤‘ì—ì„œ ì–´ë–¤ classì˜ centroidsì™€ ê°€ì¥ ê°€ê¹Œìš´ì§€ Open-world dataì˜ V_directì™€ ë¹„êµë¥¼ í•˜ëŠ” ê²ƒì´ë‹¤. ê°€ì¥ ê°€ê¹Œìš´ classì— ëŒ€í•´ì„œ(Î³ ì‘ìŒ) V_metaì— í° ê°’ì„(1/Î³í¼) ê³±í•´ ì£¼ê²Œ ëœë‹¤. 
  - ì´ê²ƒì€, encoding open classesë¥¼ ì‚¬ìš©í•˜ëŠ”ë°ì— ë” ë§ì€ ë„ì›€ì„ ì¤€ë‹¤.

- e (concep selector)

  - head-dataì˜ V_directëŠ” ì´ë¯¸ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤. tail-dataì˜ V_directëŠ” ìƒëŒ€ì ìœ¼ë¡œ less sementicí•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤. 
  - ë”°ë¼ì„œ ì–´ë–¤ ë°ì´í„°ì´ëƒì— ë”°ë¼ì„œ V_memoryë¥¼ ì‚¬ìš©í•´ì•¼í•˜ëŠ” ì •ë³´ê°€ ëª¨ë‘ ë‹¤ë¥´ë‹¤. ì´ëŸ° ê´€ì ì—ì„œ e (nn.Linear(feat_dim, feat_dim)) ë ˆì´ì–´ë¥¼ í•˜ë‚˜ ì¶”ê°€í•´ì¤€ë‹¤. 
  - ë”°ë¼ì„œ eëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.  <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126214446182.png?raw=tru" alt="image-20210126214446182" style="zoom:80%;" />

- dynamic meta-embedding <u>**facilitates feature sharing**</u> between head and tail classes



## 3-2. Modulated Attention

- **Modulated attention** : encourages different classes to use different contexts(attentions), which helps maintain **the <u>discrimination between head and tail</u>** classes.
  - <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126215731407.png?raw=tru" alt="image-20210126215731407" style="zoom: 80%;" />
- V_directë¥¼ headì™€ tail class ì‚¬ì´, ê·¸ë¦¬ê³  intra-classì‚¬ì´ì˜ ì°¨ì´ë¥¼ ë” í¬ê²Œ ë§Œë“¤ì–´ ì£¼ëŠ” ëª¨ë“ˆì´ë‹¤. ìœ„ì˜ ì´ë¯¸ì§€ì—ì„œ fê°€ f^(att)ê°€ ë˜ë¯€ë¡œì¨, ì¢€ ë” ìì‹ ì˜ classì— sementicí•œ featureë¥¼ ë‹´ê²Œ ëœë‹¤. ì´ attentionëª¨ë“ˆì„ ì‚¬ìš©í•´ì„œ fì— spatial and different contextsë¥¼ ì¶”ê°€ë¡œ ë‹´ê²Œ ëœë‹¤.
- ì•„ë˜ì˜ Attention ê°œë…ì€ ë…¼ë¬¸ì´ë‚˜, ì½”ë“œë¥¼ í†µí•´ í™•ì¸
  - SA : self-correlation, contextual information [56]
  - MA : conditional spatial attention [54]
- ![image-20210126223121413](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126223121413.png?raw=tru) ì—¬ê¸°ì„œ fëŠ” CNNì„ í†µê³¼í•œ classifierë“¤ì–´ê°€ê¸° ë°”ë¡œ ì „.
- ì´ ê°œë…ì€ ë‹¤ë¥¸ ì–´ë–¤ CNNëª¨ë“ˆì— ì¶”ê°€í•´ë”ë¼ë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ” flexibleí•œ ëª¨ë“ˆì´ë¼ê³  í•œë‹¤.



## 3.3 Learning

- cosine classifier [39, 15]ë¥¼ ì‚¬ìš©í•œë‹¤. í•´ë‹¹ ë…¼ë¬¸ 2ê°œëŠ” few-shot ë…¼ë¬¸ì´ë‹¤.
- ì´ ë°©ë²•ì€ ì•„ë˜ì˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. V_metaì™€ classifierì˜ weightê¹Œì§€ normalizeí•œë‹¤.
  - <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126223459717.png?raw=tru" alt="image-20210126223459717" style="zoom: 80%;" />
  - ì´ëŸ¬í•œ normalizeì— ì˜í•´ì„œ,  vectors of small magnitudeëŠ” ë” 0ì— ê°€ê¹Œì›Œì§€ê³ , vectors of big magnitudeëŠ” ë” 1ì— ê°€ê¹Œì›Œ ì§„ë‹¤. the reachability Î³ ì™€ ìœµí•©ë˜ì–´ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ë‚¸ë‹¤ê³  í•œë‹¤.



## 3.4 Loss function

- <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126223657868.png?raw=tru" alt="image-20210126223657868" style="zoom:67%;" />
- cross-entropy classification loss, large-margin loss 
- ë‚´ ìƒê°. ìœ„ ì‹ì˜ v^metaëŠ” classificationì´ ëœ ê²°ê³¼ë¥¼ ë§í•˜ëŠ” ê²ƒì¼ ê²ƒì´ë‹¤. vector_metaê°€ ì•„ë‹ˆë¼.
- ì˜¤ë¥¸ìª½ lossí•­ì„ í†µí•´ì„œ, the centroids {ci} K i=1 ë¥¼ í•™ìŠµ ì‹œí‚¨ë‹¤.
- ìì„¸í•œ loss í•¨ìˆ˜ëŠ” ë¶€ë¡ ì°¸ì¡°
  - <img src="https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora/image-20210126223905835.png?raw=tru" alt="image-20210126223905835" style="zoom:60%;" />



# 4. Experiments

1. Datasets
   - Image-Net2012ë¥¼ ì‚¬ìš©í•´ì„œ headì™€ tailì„ êµ¬ì„±ì‹œí‚¨ë‹¤. 115.8kì¥ì˜ ì´ë¯¸ì§€. 1000ê°œì˜ ì¹´í…Œê³ ë¦¬. 1ê°œì˜ ì¹´í…Œê³ ë¦¬ì— ìµœëŒ€ ì´ë¯¸ì§€ 1280ê°œ, ìµœì†Œ ì´ë¯¸ì§€ 5ê°œë¡œ êµ¬ì„±í•œë‹¤. 
   - Opensetì€ Image-Net2010ì„  ì‚¬ìš©í•˜ì˜€ë‹¤. 
2. Network Architecture - ResNet ì‚¬ìš©
3. Evaluation Metrics
   - the closed-set (test set contains no unknown classes) 
   - the open-set (test set contains unknown classes)
   - Trainì„ ì–¼ë§ˆë‚˜ ë°˜ë³µí•´ì„œ ì‹œì¼°ëŠ”ì§€ì— ë”°ë¼ì„œ, many-shot classes / medium-shot classes / few-shot classesë¥¼ ê¸°ì¤€ìœ¼ë¡œ accuracyë¥¼ ë¹„êµí•´ ë³´ì•˜ë‹¤. 
   - For the open-set setting, the F-measure is also reported for a balanced treatment of precision and recall following [3]. (í˜¹ì‹œ ë‚´ê°€ Open-setì— ëŒ€í•œ  accuracy í‰ê°€ë¥¼ ì–´ë–»ê²Œ í•˜ëŠ”ì§€ ê¶ê¸ˆí•´ ì§„ë‹¤ë©´ ì´ measureì— ëŒ€í•´ì„œ ê³µë¶€í•´ë´ë„ ì¢‹ì„ ë“¯ í•˜ë‹¤.)
4. Ablation Study / Result Comparisons / Benchmarking results

