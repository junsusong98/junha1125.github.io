---
layout: post
title: 【Transformer】Attention Is All You Need 공부 필기
---

- **논문** : [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- 이 논문은 그리 자세하지 않다. 
- 직관적인 이해를 위해서, 이곳 저곳에서 글을 찾아 읽고 공부했다. 
- 과거의 Post([Attention Mechanism Overveiw](https://junha1125.github.io/blog/artificial-intelligence/2021-01-17-Attention/)) 에서 정리한 내용에 추가적으로 공부한 내용이다.
- ViT (16x16 Vision Transformer ) 논문에서는 Encoder만을 사용했다. 따라서 Multi Head Attention 모듈만 이해하면 됐지만, DETR에서는 Decoder까지 이해하기 때문에 Decoder까지의 완벽한 이해가 필요했다.
- 참고 자료
  1. 과거의 Post([Attention Mechanism Overveiw](https://junha1125.github.io/blog/artificial-intelligence/2021-01-17-Attention/))에 적혀있는 Reference
  2. Youtube1 : [https://www.youtube.com/watch?v=mxGCEWOxfe8](https://www.youtube.com/watch?v=mxGCEWOxfe8)







![T3.png](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-1/Transformer_note3.png?raw=true)



---

---

- **아래 필기 공부 순서 : 맨 아래 그림 -> 보라색 -> 파랑색 -> 대문자 알파벳 매칭 하기**

![T3.png](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-1/Transformer_note8.png?raw=true)

![T3.png](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-1/Transformer_note9.png?raw=true)

![T3.png](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-1/Transformer_note10.png?raw=true)









---

---



![T3.png](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-1/Transformer_note2.png?raw=true)

![T5.png](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-1/Transformer_note5.png?raw=true)

![T4.png](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-1/Transformer_note4.png?raw=true)











