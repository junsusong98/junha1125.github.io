---
layout: post
title: 【Pytorch】DETR code teardown reports 
---

- DETR github : [https://github.com/facebookresearch/detr](https://github.com/facebookresearch/detr)
- Remarks
  1. Dockerfile 제공 (사용하지 말기)
  2. Demo code 제공
- 한글 주석 포함 github 링크
  - [https://github.com/junha1125/DETR-with-Kcomment](https://github.com/junha1125/DETR-with-Kcomment)
  - data 폴더 제외 cp 명령하기 `$ cd ~/DETR-with-Kcomment && rsync -av --progress /workspace/* ./ --exclude 'coco'` 
- 의문 정리 (검색하기)
  - demo에서 0.1을 왜 곱해주지??
- 의문 해결
  - detr.py에 `num_classes = 91 if args.dataset_file != 'coco' else 20` 를 `num_classes = 20 if args.dataset_file != 'coco' else 91` 로 바꾸면, 왜 main.py 에서 디버깅 실행하면 `from models import build_model` 에서 에러가 나는 거지? 에러내용은 아래 사진 ([해결](https://github.com/facebookresearch/detr/issues/358) - 반성하자)  
- 추신
  - 아래의 목차 순서 잘 분리해놓았으니, 무시하지 말고 꼭 신경쓰기



# 0. 핵심 배운점

- **나의 현재 문제점**
  1. 코드(main.py)를 처음부터 똑바로 안봤다. [torch.utils.data.dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)이 아니라, build_dataset이었는데.. 아무 생각 없이 패스했다. 
  2. dataloader의 파라미터 중에서 collate_fu이 정말 중요했는데, 이것도 무시했다. 
  3. 이건* 나중에 보자!라는 마음으로 패스하고, 나중에 그걸* 안봐서 너무 힘들었다. 
  4. 내가 지금 뭐하지는 모르고 계속 했던거 또하고 했던거 또했다.
- **해결책**
  1. 정말 하나하나! 꼼꼼히! 순서대로! 부셔야 한다. 
  2. 그냥 아무 생각없이 디버깅 하기 전에 아래의 방법** 중 지금의 에러를 해결하기 위해 사용해야하는 방법이 무엇인지 진지하게 생각하고! 들어가자. 손부터 움직이지 말고 머리를 쓰자. 
  3. 좌절하지 말자. 포기하지 말자. 할 수 있다. 아자아자
- **최적의 디버깅 방법\*\*이 무엇인가?**
  - 코드 직접 수정하기
  - pdb 이용하기 / pdb의 interact 이용하기
  - python debuger 이용하기 / Watch, Debug console이용하기
  - (위의 방법 섞기)
  - 코드를 직접 수정하기 + pdb이용하기
  - 코드를 직접 수정하기 + Python debuger를 이용하기
- 신경망에 있어서 디버깅 방법
  - \_\_init\_\_ 과 forward가 실행되는 시간 차는 매우 다르다. 따라서 분리해서 생각하도록 하자.



---

# 1. detr_demo.py. Inference 흐름 따라가기

- [Github/facebookresearch/detr Code link](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb)

- Error 해결     

  ```sh
  $ pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html 
  # docker Image 그대로 사용했어야 했다. requirements.txt가 개세끼다
  $ conda install -c conda-forge ipywidgets
  $ pip install pdbpp
  ```

- 주요 코드 설명

  1. `h = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1), self.query_pos.unsqueeze(1)).transpose(0, 1)` 
     - 위의 transformer 함수 내부 : (`(625, 1, 256) + 0.1 * (625(갯수), 1, 256(백터)` , `(100, 1 ,256)`) : **0.1을 왜 곱해주지??**
     - transformer ouput : `(100, 1 ,256)`
     - h.shape : (by transpose) : `(1, 100, 256)`

- 이 코드는 nn.transformer를 그대로 사용하기 때문에.. 여기서 그만 공부..

![SmartSelect_20210418-161421_Samsung Notes.jpg](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-4/detrcode/SmartSelect_20210418-161421_Samsung%20Notes.jpg?raw=true)



---

# 2. Evaluation. Inference 흐름 따라가기 

- `main.py`는 train.py 코드도 되고, test.py 코드도 된다. `--eval` 옵션만 넣고 빼주면 된다.

```sh
[Terminal]
$ python main.py \
		--batch_size 1\
        --no_aux_loss\
        --eval\
        --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth
        --coco_path /workspace/coco # /path/to/coco

$ python main.py \
       	--batch_size 2\
        --no_aux_loss\
        --eval\
        --resume checkpoints/detr-r50-e632da11.pth\
        --num_workers 4\
        --world_size 2\
        --coco_path /dataset/coco\
        --output_dir result

[Vscode Debugging] 
# ./vscode/launch.json
"args" : ["--batch_size", "2", 
        "--no_aux_loss", 
        "--eval", 
        "--resume", "checkpoints/detr-r50-e632da11.pth", 
        "--num_workers", "4",
        "--world_size", "2",
        "--coco_path", "/dataset/coco",
        "--output_dir", "result"]
```



## 2.0 `main.py` 분석해보기

- `print(args)`       	

  ```sh
  - Namespace(aux_loss=False, backbone='resnet50', batch_size=1, bbox_loss_coef=5, clip_max_norm=0.1, coco_panoptic_path=None, coco_path='/workspace/coco', dataset_file='coco', dec_layers=6, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dropout=0.1, enc_layers=6, eos_coef=0.1, epochs=300, eval=True, frozen_weights=None, giou_loss_coef=2, hidden_dim=256, lr=0.0001, lr_backbone=1e-05, lr_drop=200, mask_loss_coef=1, masks=False, nheads=8, num_queries=100, num_workers=2, output_dir='', position_embedding='sine', pre_norm=False, remove_difficult=False, resume='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth', seed=42, set_cost_bbox=5, set_cost_class=1, set_cost_giou=2, start_epoch=0, weight_decay=0.0001, world_size=1)
  ```

- 코드 순서 요약하기    

  1. args 설정 및 seed 설정
  2. model, criterion, postprocessors = build_model(args)
  3. 총 파라메터 갯수 계산 하기
  4. backbone / Transformer-encoder, decoder / detector head 각각의 learning rate를 다르게 주기
  5. optimizer와 lr_scheduler 설정
  6. data_loader 만들어 주기 - 1. dataset 정의 , 2. batchSampler 정의 
  7. coco_api 를 사용한 dataset validation = evaluate AP 계산하는 방법
  8. Model Parameters load 하기(1) - frozen_weights 경로가 있을 경우 (= panoptic segmentaion 모듈만 학습시키고 싶은 경우)
  9. Model Parameters load 하기(2.1) - Evaluate 하기 위한 파라메터
  10. Model Parameters load 하기(2.2) - Train 하기 위한 파라메터
  11. Evaluation만 하고 코드 종료(return) (1) model infernece하기 (2) coco api로 AP 구하기
  12. If args.eval=False 이라면, 바로 위 코드 안하고, Traiining 시작하기



## 2.1 datasets/coco.py /build_dataset()

- `torchvision.datasets.CocoDetection` 를 상속한다. ([링크](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CocoDetection)) 여기서! `target_transform` 가 의미하는 것은, (Image, label) 쌍에서 image augmentation만해주는게 아니라, label에 대한 변형(agumentation)까지도 해주는 것을 의미한다.

- `dataset/coco.py `안의 `make_coco_transforms`함수에서 `Transforms(agumentation)`을 정의한다. 이때 `dataset/transforms.py` 에 정한 Transformer만을 사용한다.  여기서 `Transforms(agumentation)`을 정의하는 것을 보면, 다 공통적으로 아래와 같은 구조이다. 따라서 내가 구현하고 싶은 Agumentation이 존재한다면 참고해서 만들도록 하자.    

  ```python
  class New_Transformer_Name(object):
      def __init__(self, p=0.5):
          # just 맴버 변수 정의
  	def __call__(self, img, target=None):
          return img, target
  ```

- Backbone에 이미지가 들어가기 전에, DataLoader에서 나오는 변수의 타입이 type(Image, label) = (torch.tensor, dict) 이었다가 (NestedTensor, dict) 으로 바뀐다. 그 이유는 torch.utils.dataloader의 파라메터인 `collate_fn=utils.collate_fn` 덕분이다.

- dataset_val에는 build_dataset()으로 부터 받은 `torch.utils.dataset`클래스이다. 따라서 transforms가 적용된 후! `collate_fu`이 적용되는 것이다.       

  ```python
  ## main.py 아래의 collate_fn=utils.collate_fn !! 덕분이다.
  data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,
                                   drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)
  ## util/misc.py
  def collate_fn(batch):
      # batch[0] : len이 batch_size인 tuple. Image 정보 들어가 있다. batch[0][0]에는 torch tensor가 들어가 있다.
      # batch[1] : len이 batch_size인 tuple. label 정보 들어가 있다. batch[1][0]에는 dict()가 들어가 있다. dict_keys(['boxes', 'labels', 'image_id', 'area', 'iscrowd', 'orig_size', 'size'])
      batch = list(zip(*batch))
      batch[0] = nested_tensor_from_tensor_list(batch[0])
      return tuple(batch)
  
  def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):
      return NestedTensor(tensor, mask)
  
  ## datasets/transforms.py
  def resize(image, target, size, max_size=None):
      """
      이미지에서 (w,h 중) 작은 쪽을 size(val일때 800) 크기로 변환해준다. 
      작은 쪽을 800으로 변한해서 큰쪽이 max_size보다 커치면, 큰쪽이 max_size가 되도록 이미지를 변환한다.
      """
  ```

- `collate_fn` 함수는 `misc.py의 Nested_tensor_from_tensor_list()` 함수에 의해서 정의 되어있다.   
  ![SmartSelect_20210418-161447_Samsung Notes.jpg](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-4/detrcode/SmartSelect_20210418-161447_Samsung%20Notes.jpg?raw=true)



## 2.2 models/detr.py 

- 아래로 내려가면서, 함수가 호출된다. build에서 핵심적인 내용반 뽑았다. 

```python
# main.py
from models import build_model
model, criterion, postprocessors = build_model(args)

# models/detr.py
def build(args):
    backbone = build_backbone(args)
    transformer = build_transformer(args)
    model = DETR(backbone, transformer, num_classes=num_classes, num_queries=args.num_queries, aux_loss=args.aux_loss)
    matcher = build_matcher(args)
    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=args.eos_coef, losses=losses)
    postprocessors = {'bbox': PostProcess()}
    return model, criterion, postprocessors
```

따라서 아래의 과정을 따라서 차근차근 공부해나갈 예정이다. 

1. build_dataset()
2. build_backbone()
3. build_transformer()
4. model = DETR()
5. build_matcher()
6. SetCriterion()
7. PostProcess()



## 2.2.1 models/position_encoding.py /build_position_encoding()

- 2가지 방법 embeding 방법이 존재한다.]

1. Leaned - Positional Embeding
   - `nn.Embedding`, `nn.init.uniform_` 를 사용해서 row_embed, col_embed 변수 정의하기
   - `pos = torch.cat([x_emb.unsqueeze(0).repeat(h, 1, 1),y_emb.unsqueeze(1).repeat(1, w, 1)], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)`
   - (demo) 최종 pos의 shape : (N장, 256, 25, 25) 여기서 25는 feature map의 width, hight
2. sinusoidal  - Positional Embeding
   - 어려울 것 없이, 아래의  4. Additions. Positional Embeding Code 와 똑같은 코드이다. 이 코드에 의해서도 shape( #Vector, Vector_dim )이 만들어진다. 아래에서 최종적으로 만들어진 pos_x , pos_y  또한 똑같다. 
   - pos_x  : shape[2, 28, 38, 128] -> 여기서 28개 모두 같은 값이고, 결국 만들어진 것은 38*128이다. (아래의 4. Additions 참조)
   - pos_y  : shape[2, 28, 38, 128] -> 여기서 38개 모두 같은 값이고, 결국 만들어진 것은 28*128이다. (아래의 4. Additions 참조)
   - (main.py debug) 최종 pos의 shape : (N장, 256, 28, 38) 여기서 25는 feature map의 width, hight   
     ![SmartSelect_20210418-161517_Samsung Notes.jpg](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/2021-4/detrcode/SmartSelect_20210418-161517_Samsung%20Notes.jpg?raw=true)



## 2.2.2 models/backbone.py /build_backbone()

```python
def build_backbone(args):
    # 핵심 module 정의 1. position_embedding
    position_embedding = build_position_encoding(args)
    
    # 핵심 module 정의 2. backbone
    train_backbone = args.lr_backbone > 0
    return_interm_layers = args.masks
    backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation) # torchvision.models

    # nn.Sequential 로 2개의 모듈 묶어주기
    model = Joiner(backbone, position_embedding)

    # model의 type은 nn.Sequential 이기 때문에, 아무런 맴버변수가 없다. 아래에 의해서 클래서 맴버변수 하나 만들어 놓음
    model.num_channels = backbone.num_channels
    return model


class Joiner(nn.Sequential):
    def __init__(self, backbone, position_embedding):
        super().__init__(backbone, position_embedding)

    def forward(self, tensor_list: NestedTensor):
        xs = self[0](tensor_list) # self.backbone(tensor_list)
        # xs['0'].tensors.shape = torch.Size([2, 2048, 28, 38])
        # 만약에 resnet 중간에 feature map을 뽑아 왔다면, xs['1'], xs['2'], xs['3'] 순차적으로 저장된다. 
        out: List[NestedTensor] = []
        pos = []
        for name, x in xs.items():
            out.append(x)
            # position encoding
            pos.append(self[1](x).to(x.tensors.dtype)) # self.position_embedding(x)

        # 여기서 0은 backbone에서 가장 마지막 C4 layer에서 뽑은 결과
        # out[0].tensors.shape = torch.Size([2, 2048, 28, 38]). out[0].mask 있음
        # pos[0].shape = torch.Size([2, 256, 28, 38])
        return out, pos
```





## 2.2.3 models/transformer.py /build_transformer()

```python
def build_transformer(args):
    return Transformer()

class Transformer(nn.Module):
	def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,
                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,
                 activation="relu", normalize_before=False,
                 return_intermediate_dec=False):

        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,
                                                dropout, activation, normalize_before)
        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None
        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)

        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,
                                                dropout, activation, normalize_before)
        decoder_norm = nn.LayerNorm(d_model)
        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,
                                          return_intermediate=return_intermediate_dec)
        
        # 모델의 전체 파라미터 초기화를 아래와 같이 한다. 잘 알아두기.
        self._reset_parameters()
	def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
```

![image-20210418173318543](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora-rcv/image-20210418173318543.png?raw=tru)

1. 모델 전체 파라미터 초기화 `nn.init.xavier_uniform_(p)`
2. 위의 코드처럼, `models/transformer.py`에 정의된 아래의 함수들을 순차적으로 사용한다. 차근차근 알아가보자. 
   1. TransformerEncoderLayer
   2. TransformerEncoder
   3. TransformerDecoderLayer
   4. TransformerDecoder
3. 코드 전체에서 `attn_mask` `tgt_mask` 변수는 사용되지 않는다.(=`None` 이다) 이 Mask 기법은 All you need attention 논문에서, decoder의 sequencial한 input과 output을 구현해 학습시키기 위해서 사용되는 변수이다. 자세한 내용은 아래 4. Attention 부분의 내용을 참조할 것.
4. 대신 `key_padding_mask` 변수 은 사용한다. 배치에 들어간 모든 이미지를 한꺼번에 처리하기 위한 Transformer를 구현하기 위해, 배치 속에서 상대적으로 작은 이미지는 pading된다. 이 pading된 값에 대해서는 Attention을 구하면 안되므로, Mask처리를 해준다. Value도 0이다. 위의 `Nested_tensor_from_tensor_list` 내용에 있는 손 그림 참조.



![image-20210418173519152](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora-rcv/image-20210418173519152.png?raw=tru)

1. MultiheadAttention

   - **torch.nn.module.MultiheadAttention**
   - 사용 예시 `self.self_attn = nn.MultiheadAttention(d_model=512, nhead=8, dropout=dropout)`    
     ![image-20210418193545367](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora-rcv/image-20210418193545367.png?raw=tru)

2. TransformerEncoderLayer

   - `MHA 이나 Feed Forward하기 전에 Normalization 하기` = forward_pre로 구현
   - `MHA 이나 Feed Forward한 후에 Normalization 하기` = forward_post로 구현   
     ![image-20210418200621077](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora-rcv/image-20210418200621077.png?raw=tru)
   - `dropout`은 임의로 노드를 잠궈버리는 것. 즉 Zero로 만들어 버리는 것이다. 따라서 dropout의 forward를 위한 파라미터는 노드를 넣어줘야한다. (코드 예시. `src = src + self.dropout1(src2)`)

3. TransformerEncoder

   - 같은 TransformerEncoderLayer를 서로 다른 layer로 구성하기 위해서 아래와 같은 코드기법을 사용했다.    

     ```python
     def _get_clones(module, N):
         return nn.ModuleList([copy.deepcopy(module) for i in range(N)])
     
     class TransformerEncoder(nn.Module):
         def __init__(self, encoder_layer, num_layers, norm=None):
             super().__init__()
             self.layers = _get_clones(encoder_layer, num_layers)
             
         def forward(self, src,
                     mask: Optional[Tensor] = None,
                     src_key_padding_mask: Optional[Tensor] = None,
                     pos: Optional[Tensor] = None):
             output = src
             for layer in self.layers:
                 output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos)
     ```

4. TransformerDecoderLayer

   - `MHA 이나 Feed Forward하기 전에 Normalization 하기` = forward_pre로 구현
   - `MHA 이나 Feed Forward한 후에 Normalization 하기` = forward_post로 구현 
   - 그림은 나중에 추가할 예정(코드 미리 쪼개고 분석해놨으므로 금방한다.)

5. TransformerDecoder

   - ㅇㅇ



## 2.2.4 models/detr.py /DETR()



## 2.2.5  models/matcher.py /build_matcher()



## 2.2.6 models/detr.py /SetCriterion()



## 2.2.7 models/detr.py /PostProcess()















---

# 3. New modules

1. **torch.repeat(sizes), torch.expand**

   - sizes (torch.Size or int...) – 각 차원에 대한 반복 수 (The number of times to repeat this tensor along each dimension)     

   ```sh
   >>> x = torch.tensor([1, 2, 3])
   >>> x.repeat(4, 2)
   tensor([[ 1,  2,  3,  1,  2,  3],
           [ 1,  2,  3,  1,  2,  3],
           [ 1,  2,  3,  1,  2,  3],
           [ 1,  2,  3,  1,  2,  3]])
   >>> x.repeat(4, 2, 1).size()
   torch.Size([4, 2, 3])
   
   self.col_embed[:W].unsqueeze(0) >> torch.Size([1, 25, 128])
   self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1) >> torch.Size[25,25,125] 
   ```

2. **torch.flatten(input, start_dim=0 : int, end_dim=-1 : int) → Tensor**

   - h : (1, 256,25,25)
   - h.flatten(2) = h.flatten(start_dim = 2) : (1, 256, 625)

3. **torch.permute(2,0,1)**

   - 차원 치환/교환

   -  (1, 256, 625) ->  (625, 1, 256)

4. **torch.unsqueeze(d)** 

   - d: unsqueeze하고 싶은 차원
   - d =1 -> (625, 256) -> (625, 1,256) 

5. **torch.transpose(*input*, *dim0*, *dim1*)**

   - 딱 2개의 차원을 교환한다

6. **torch.nn.Linear(in_features_int, out_features_int)**

   - forward에 들어갈 수 있는 shape는, 무조건 in_feature_int 차원의 1차원 백터가 아니다!! 아래의 차원이 될 수 있다.
   - Input : `(N,*,H_in)`, 이렇게만 forward로 들어가면 된다. Output: `(N, *, H_out)` 
   -  여기서 `H_in = in_features :int`, `H_out = out_features :int` 이다.
   
7. **argparse**

   - config 파일을 사용해서 환경변수를 저장하기도 하지만, 여기서는 간단하게 argparse만으로 환경변수를 정의했다.
   - `main.py`의 argparse 기법을 사용하면, `args.junha = 1` 만 코드에 추가해줘도 자동으로 새로운 key, valuse가 생성된다.

8. **torch.tensor.numel()**

   - tensor 안에 들어 있는 파라메터 갯수 자연수로 반환

9. **nn.Module.named_parameters()**, **nn.Module.parameters()**

   - iteration 이다. 
   - `[p.shape for p in model.parameters()]`, `[p for p in model.parameters()]` 이런식으로 출력해서 보기
   - `[n for n, p in model_without_ddp.named_parameters() if "backbone" in n and p.requires_grad]`
   - `[(n,p) for n, p in model.named_parameters()]`   
     ![image-20210408133142137](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora-rcv/image-20210408133142137.png?raw=tru)

10. **nn.Embedding**

    - [torch.nn.Enbedding documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)
    - `self.row_embed = nn.Embedding(50, 128)`
    - `self.row_embed.weight.shape` = torch.Size([50, 128])

11. **nn.init.uniform_**

    - [torch.nn.init](https://pytorch.org/docs/stable/nn.init.html) 여기에 모델 weight를 init하는 방법이 다 있다. 사용은 아래와 같이 한다.
    - `nn.init.uniform_(self.row_embed.weight)`
    
12. torch.tensor.cumsum(input, dim)

    - dim 차원으로 바라봤을 때, 나오는 1차원 백터들에 대해서 누적된 합을 계산한 텐서 반환.
    - [1, 2, 3] -> [1, 3, 6]

13. nn.Dropout(p)

    - p = 노드를 zero로 만들어 버릴 가능성 
    - `dropout`은 임의로 노드를 잠궈버리는 것. 즉 Zero로 만들어 버리는 것이다. 따라서 dropout의 forward를 위한 파라미터는 노드를 넣어줘야한다. (코드 예시. `src = src + self.dropout1(src2)`)



---

# 4. Additions

## 4.1 **논문 외 보충 내용**

1. Attention is All you need의 논문 내용 정리! 같은 색 매칭해서 보기! - 그리기 코드는 맨 아래에 참조    
   ![image-20210416222918201](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora-rcv/image-20210416222918201.png?raw=tru)

   - Sinusoidal  - Positional Embeding Code    

     ```python
     import numpy as np
     d_vector = 256
     num_vector = 625
     PEs = np.empty((num_vector,d_vector))
     
     period = 1000
     for i in range(num_vector):
         if i%2 == 0:
             w = 1/ period**(2*i/d_vector) # 상수. for문에 의해서 num_vector개 만들어짐
             pos = np.arange(256)
             PEs[i] = np.sin(pos*w)
         if i%2 != 0:    
             w = 1/ period**(2*i/d_vector)
             pos = np.arange(256)
             PEs[i] = np.cos(pos*w)
     
     %matplotlib inline
     import matplotlib.pyplot as plt
     from matplotlib.pyplot import figure
     
     figure(figsize=(25, 23), dpi=80, facecolor='black')
     imgplot = plt.imshow(PEs)
     plt.colorbar()
     ```

2. Multi head self attention에서 **Mask** 개념, **Masked** Self-attention

   - Reference
     1. Transformer를 정말 **완벽**하게 표현설명한 [외국 블로그글](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)
     2. Transformer 코드 잘 만들어 놓은 [한국 블로그글](https://paul-hyun.github.io/transformer-02/)
     3. Transformer Architecture를 도식그림으로 잘 그려놓은 [한국 블로그글](https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225)
     4. Teacher forcing을 잘 설명해 놓은 [한국 블로그글](https://lv99.tistory.com/26) 
   - NLP에서 Mask는 Decoder에서 사용되어야 하는 Layer이다. Decoder에서 3번쨰 단어(fine)를 예측할 때, Decoder의 (Emcoder 결과 말고) Input(I, am)으로 2개의 단어가 들어간다. 학습할 때 우리는 Input(I, am, find)를 모두 가지고 있으므로, find은 가린체로 decoder의 Input으로 넣어줘야한다. 그게 Mask가 하는 역할이다.    
   -  Mask가 처리되어야 하는 부분은 다른 곳이 아니라 저 부분만 이다. 그래서 Masked 개념은 Decoder에서만 사용된다.    ![image-20210417165538067](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora-rcv/image-20210417165538067.png?raw=tru)
   - Mask를 이용한 연산 처리 방법    
     ![image-20210417170332292](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora-rcv/image-20210417170332292.png?raw=tru)



## 4.2 추가 추신

1. **COCO dataset 다운로드 하기**    

   ```sh
   $ apt-get install wget
   $ wget https://gist.githubusercontent.com/mkocabas/a6177fc00315403d31572e17700d7fd9/raw/a6ad5e9d7567187b65f222115dffcb4b8667e047/coco.sh
   $ sh coco.sh
   ```

2. **dockerfile** 

   - 그대로 사용했다가, torchvision 버전 엉망됐다. `requirements.txt`에서 torch, torchvision, sumitit 삭제하고 설치하자.
   - `$ pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html` 
   - docker-hub의 `-runtime` , `-devel` 태크 분석하기. 지금 내가 쓰고 있는게 `pytorch/pytorch:1.5-cuda10.1-cudnn7-runtime`이다. 아래의 단서로 runtime은 그냥 가볍게 만든 우분투, devel은 최대한 다 넣어놓은 우분투 라고 예측할 수 있다.
     1. 지금 runtime image에 wget도 설치가 안되어 있다. 
     2. docker-hub를 보면 runtime은 3.3G 정도, devel은 5.3G 정도이다. 

3. **vscode에서 pdb이용하기**    
   ![image-20210406212137524](https://github.com/junha1125/Imgaes_For_GitBlog/blob/master/Typora-rcv/image-20210406212137524.png?raw=tru)

   - `$ pip install pdbpp`

4. **vscode python debugging debug args setting 설정**

   - `,"args": ["--batch_size", "1", "--no_aux_loss", "--eval", "--resume", "https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth", "--coco_path", "/workspace/coco"]`

   - 위의 내용을, `/pythonPackage/.vscode/launch.json`의 config 파일에 넣어두기     

     ```sh
     {
          "version": "0.2.0",
          "configurations": [
              {
                  "name": "Python: Current File",
                  "type": "python",
                  "request": "launch",
                  "program": "${file}",
                  "console": "integratedTerminal",
                  "debugOptions" : ["DebugStdLib"],
                  "justMyCode": false,
                  "args" : ["--batch_size", "1", "--no_aux_loss", "--eval", "--resume", "https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth", "--coco_path", "/workspace/coco"]
                }
          ]
      }
     ```

5. Pytorch 분산학습

   - rank, world size 을 detr에서 설정해줘야 Multi-GPU사용이 가능한데, 이게 뭔지모르니까 막무가네로 확경변수 설정 못하겠다. 
   - 필요하면 다음의 Tuturial을 공부하자. [Tuto1](https://pytorch.org/tutorials/beginner/dist_overview.html) [Tuto2](https://pytorch.org/docs/stable/distributed.html)

