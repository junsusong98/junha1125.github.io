---
layout: post
title: 【기계학습】 기계학습 핵심정리 chap1, chap2, chap3, chap4
description: >
    기계학습 책을 공부하고, 핵심만 기록해 놓는다. 구체적인 그림 및 설명은 최소화 하고, 꼭 필요한 설명은 1줄 이내로 적는다. 
    마음가짐 : '책 100번 볼거다. 또 보면 된다. 핵심만 정리해라'
---

기계학습 책 핵심 정리 chapter 1,2,3,4

# Chapter 01 - 소개

- 1.1절 - 기계학습이란

- 1.2절 - feature(input) demention의 가시화 생각해보기

- 1.3절 - 특징 공간 변환 공식 찾기 = 신경망 학습 알고리즘

- 1.4절 - 기계 학습 알고리즘 - Θ = argmin_Θ loss(Θ)

- **1.5절 - 모델선택**
    1. 1d -> 1d로 이해하는 underfitting과 overfitting
    2. bias와 variance   
        - Train Data set의 변화에 따른 모델들 간의 관계
    3. validation set과 cross-validation
        - validation 알고리즘 흐름 참조(50p 알고리즘1-4)

- **1.6절 - 규제**
    1. data augmentation
    2. weight decay 
        - Loss = L1(x,y) + wights

- **1.7절 - 기계 학습 유형**
    1. 지도, 비지도, 준비도, 강화
    2. oneline/offline  
        deterministic/stochastic  
        discriminative/generative  

- 1.8절 - 기계학습의 과거, 현재, 미래
    - 다층퍼셉트론 -> SVM -> Deep learning
    - 쫄지말자. 복잡도만 다를 뿐 기본 원리는 같다. 

# Chapter 02 - 수학
## 2.1절 - 선형대수
- Norm, 단위백터, Cosine similarity(코사인 유사도=단위백터화 후 내적)
- 퍼셉트론과 행렬곱 Output(cx1) = Weight(cxd) x Input(dx1)
- 행렬에서 행백터들(혹은 열백터들)이 선형 결합, 선형독립과 선형종속
- mxn행렬에 대해 rank가 d(\<= n)이면 행백터들에 대해, d차원 백터공간을 표현할 수 있다.
- mxn행렬에 대해 최대 rank는 n(=행백터의 차원) 이다. 
- 역행렬이 없는 행렬 = singular matrix
- 역행렬을 가진다 = 최대 계수 = 모든 열 선형 독립 = 고윳값은 모두 0이 아니다.
- 모든 행렬은 (1)양의 정부호, (2)양의 준 정부호, (3)음의 정부호, (4)음의 준 정부호, (5)부정부호 행렬 5가지로 분류 할 수 있다. 분류 방법은 88p
- 이 부호를 가지는 행렬은 각각 고윳값이 (1)모두 양수, (2)0이거나 양수, (3)음수, (4)0이거나 음수, (5)양수와 음수, 이다. 
- 고윳값과 고유백터, 그리고 고유윳값 분해(약수분해, 인수분해 처럼! 정사각행렬일때), 특잇값 분해(정사각행렬이 아니어도) 
- 모든 고유 백터는 orthogonal(직교)한다.

## 2.2절 - 확률과 통계
- 확률분포, 합과 **곱의 법칙**, 조건부 확률, 두 사건의 독립
- 베이즈 정리 
    - 하얀 공이 나왔다는 사실만 알고 어느 병(a,b,c)에서 나왔는지 모르는데, 어느 병인지 추정해라. 
    - 사후 확률(사건이 벌어진 후 확률) = P( Label \| 데이터,Input Feature ) = 알기 어려움
- 우도(likelihood) = P( 알고있음,데이터 \| 추정해야함,모델/Label ) = 그나마 쉬움.
- 최대 우도법(Maximum likelihood) = argmaxP( 데이터x \| 모델 )  
    데이터 x가 주어졌을 때, x를 발생시켰을 가능성을 최대로 하는 매개변수(모델)
- 최대 로그 우도 추정
- 평균과 분산 공식, d차원 feature에 대한, d개의 평균, dxd 차원 공분산 행렬(105p)
- 가우시안 분포(=정규 분포), 베르누이 분포(1번 실험) -> 이항분포(m번 실험)
- 정보이른 : 메세지가 가진 정보량을 수량화하여 '수'로 나타낼 수 있을까?  
    발생활 확률이 적은 사건일 수록, 정보량이 크다!
- 엔트로피 : 한(P) 확률분포의 무질서도, 불확실성 측정 = 클 수록 예측하기 어렵다.(윷놀이<<주사위)
    - Cross 엔트로피 : 두(P,Q) 확률분포의 무질서도. -> 아래와 같이 분리가능 (교환법칙 성립)
    - P와 Q의 Cross 엔트로피 = P의 엔트로피(얼마나 예측하기 힘든지) + P와 Q사이의 KL diversence
    - KL diversence : 두 확률분포가 얼마나 다른지? 다를 수록 큰 수 (교환법칙 성립 안함)
    - cross 엔트로피 장점은 (Mean Squared제곱 Error 에서 처럼) 더 큰 오류에 더 낮은 벌점 주는 경우가 발생하지 않음(5장 추가 참조)

## 2.3절 - 최적화 이론
- global optimal solution(전역 최적해)/ local optimal solution(지역 최적해)
- 미분/ 편미분/ 독립변수(가중치)와 종속변수(Loss)/ 연쇄법칙
- 아코비안(각 가중치에 대한 편미분 행렬)과 헤시안(각 가중치에 대한 2차 편미분 행렬)
- 테일러 급수 - 함수는 모르고, 함수값과 미분값을 알 때.
- BGD(batch gradient decent) : 전체 데이터에 대한(1 epoch 전체) '가중치의 편미분 값'을 평균하여 가중치 갱신
- SGD(stochastic gradient decent) : 데이터 하나(1장) or 배치 묶음(1 epoch 전체 X)에 대해서 '가중치의 편미분 값'을 가지고 가중치 갱신 반복


# chap3 - multi perceptron
- 여기 내용은 차라리 밑바닥 시리즈로 공부하는게 낫겠다. 
- 3.1절 - 신경망 기초
    1. deterministic/stochastic 신경망 : 계산식에서 난수를 사용하나 안하나? 출력이 항상 같나 다르나?

- 3.2절 - 퍼셉트론

- 3.3절 - 다층 퍼셉트론
    1. 활성화 함수
        - 계단함수 - 1층 퍼셉트론
        - 로지스틱 시그모이드, 하이퍼볼릭 탄젠트 - Gradient Vanishing 문제
        - 소프트플러스, Relu 

- 3.4절 - 오류 역전파 알고리즘

- 3.5절 - 미니배치 SGD
    1. 데이터 집합에서, t개의 샘플을 무작위로 뽑아 미니배치를 구성. 미니배치에 속한 샘플로 gredient 평균 후 가중치 갱신

- 3.6절 - 다층 퍼셉트론에 의한 인식
- 3.7절 - 다층 퍼셉트론의 특성
    1. 휴리스틱의 중요성
        - 아키텍처, 초깃값, 학습률

# chap4 - Deep Learning
- 넘어간 부분은, 궁금하면 나중에 다시 공부하자. 지금은 아니다. 
- 4.1절 - 딥러닝의 등장
- 4.2절 - 깊은 다층 퍼셉트론
- 4.3절 - 컨볼루션 신경망

- 4.4절 - 컨볼류션 신경망 사례연구
    1. 

- 4.5절 - 생성 모델
    1. 

- 4.6절 - 딥러닝은 왜 강력한가?
    1. 
